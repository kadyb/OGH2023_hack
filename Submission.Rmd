---
title: "Automatic land cover mapping"
author: "Krzysztof Dyba"
date: "28 August 2023"
output:
  html_document:
    toc: yes
    toc_float: true
    css: "style.css"
---

## Data loading

```{r message=FALSE}
library("terra")
library("ranger") # Random Forest model
library("yardstick") # validation metrics
set.seed(1) # define seed of randomness
```

Let's start by listing the rasters to be loaded from the `Landast_crop` folder.
We also need to define the file extension `pattern = "\\.TIF$"` and full paths
`full.names = TRUE` in `list.files()` function. Then we load these files using
the `rast()` function.

```{r}
landsat_f = list.files("Landast_crop/", pattern = "\\.TIF$", full.names = TRUE)
landsat_f = landsat_f[-1] # remove panchromatic band (15 m)
landsat = rast(landsat_f)
```

For ease of use, we can abbreviate the band names.

```{r}
names(landsat) = paste0("B", 1:7) # change band names
landsat
```

Next, we need to scale the values to the spectral reflectance. Basically,
we change the datatype from integer to float. Note that there are outliers
(below 0 and above 1) in this dataset.

```{r warning=FALSE}
# scale data to reflectance
landsat = landsat * 2.75e-05 - 0.2
summary(landsat) # print statistics
```

We can also display RGB composition using the `plotRGB()` function. We need to
properly define the spectral bands and it is worth to improve the contrast
using the `stretch` argument.

```{r}
plotRGB(landsat, r = 4, g = 3, b = 2, stretch = "lin")
```

Now let's load our reference data with land cover categories. We have two
datasets -- a training (to train the classification model) and a validation
(to independently evaluate its performance).

```{r}
training_f = "training_set.tif"
training = rast(training_f)
training
```

Using the `levels()` function we can see what categories occur in the area.

```{r}
levels(training)[[1]]
```

We can also display a map.

```{r}
colors = c("#1445f9", "#d20000", "#29a329", "#fdd327", "#d9d9d9")
plot(training, main = "Training dataset", col = colors)
```

We can load the validation data in exactly the same way.

```{r}
validation_f = "validation_set.tif"
validation = rast(validation_f)
validation
```

```{r}
plot(validation, main = "Validation dataset", col = colors)
```

Note two things:

1. The satellite data (EPSG:32634) and land cover data (EPSG:2180) have
different coordinate reference systems (CRS).
2. The rasters are not aligned (the pixels are slightly shifted).

In the last step, we load the csv file for which we have to make a prediction
and finally submit the results. For this purpose, we can use the `read.csv()`
function.

```{r}
submission = read.csv("submission.csv")
head(submission)
```

The first two columns contain the geographical coordinates of points in the
EPSG:2180 (longitude and latitude). The third column `category` is empty and
we have to indicate what land cover category is there.

Then let's convert the data frame to a vector (SpatVector) and reproject
the coordinate reference systems to EPSG:32634.

```{r}
submission = vect(submission, geom = c("X", "Y"), crs = "EPSG:2180")
submission = project(submission, crs(landsat))
```

## Model training

To train the model, we do not have to use all available data (pixels), we can
use a random sample, which will significantly speed up training.
The `spatSample()` function is used to draw the sample and can be done as follows.

```{r}
train_smp = spatSample(training, size = 20000, method = "random",
                       as.points = TRUE, values = FALSE, na.rm = TRUE)
```

We have already sample the points (coordinates), now we need to obtain the
values of spectral bands (`landsat`) and land cover categories (`training`
and `validation`). The `extract()` function is used to extract pixel values.
Remember that the objects must have identical CRS.

```{r}
x = extract(training, train_smp, ID = FALSE)
y = extract(landsat, project(train_smp, crs(landsat)), ID = FALSE)
train_smp = cbind(x, y) # combine columns
```

Let's check what the data distribution looks like using the `table()` and 
`prop.table()` functions.

```{r}
prop.table(table(train_smp$category)) * 100
```

As we can see, our dataset is strongly unbalanced, i.e. the most pixels
represent `cropland` and `vegetation`, and the other categories appear very
rarely.

We can use the same functions to create a validation dataset.

```{r}
validation_smp = spatSample(validation, size = 20000, method = "random",
                            as.points = TRUE, values = FALSE, na.rm = TRUE)
x = extract(validation, validation_smp, ID = FALSE)
y = extract(landsat, project(validation_smp, crs(landsat)), ID = FALSE)
validation_smp = cbind(x, y)
rm(x, y) # remove unnecessary variables
```

The key step is to train the classification model based on the training data.
In this example, we will use the random forests from the **ranger** package
and the default hyperparameters. As the first argument of the function, we must
indicate the classification formula, i.e. indicate which variable is explained
(dependent) and which are explanatory (independent). Our model should classify
land cover categories according to spectral bands (B1 - B7).

```
category ~ B1 + B2 + B3 + B4 + B5 + B6 + B7
```

The above notation can be simplified by using a dot instead of the names of
explanatory variables.

```{r}
# ranger uses all threads by default
mdl = ranger(category ~ ., data = train_smp, importance = "impurity")
```

Using the `importance` argument (and later function), we can examine the
importance of variables, i.e. indicate which spectral bands are most useful
for classification and which are less important.

```{r}
barplot(sort(importance(mdl)), xlab = "Spectral band", main = "Variable importance")
```

## Model evaluation

After the model is trained, its performance should be checked on an independent
dataset, i.e. one that was not used for training. There are many ways to
[validate](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), but
in this example we will use the holdout method (`validation_smp` data).

We use the `predict()` function to make a prediction on a new dataset for
which we know the actual land cover categories (of course, they should be
removed in the prediction).

```{r}
validation_pr = predict(mdl, validation_smp[, -1])
validation_pr = validation_pr$predictions # select predictions only
head(validation_pr)
```

Now we need to calculate the model performance metrics -- there are more than
a [dozen](https://en.wikipedia.org/wiki/Precision_and_recall) of them.
The simplest is accuracy, which is the ratio of correct classifications to
all classifications (correct and incorrect), but in our case it will
[unreliable](https://en.wikipedia.org/wiki/Precision_and_recall#Imbalanced_data)
because our dataset is unbalanced. A better alternative would be balanced 
accuracy or [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa).

```{r collapse=TRUE}
# accuracy is over optimistic (don't use this)
accuracy_vec(validation_smp$category, validation_pr)

# balanced accuracy
bal_accuracy_vec(validation_smp$category, validation_pr)

# Cohen's kappa
kap_vec(validation_smp$category, validation_pr)
```

Moreover, we can do a more thorough analysis using
[confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) and see
which classes are correctly classified and which are incorrectly.

```{r}
# confusion matrix
table(prediction = validation_pr, true = validation_smp$category)
```

## Submission

Finally, we need to make a prediction for the `submission` object.
We can crop the satellite scene to the extent of the point layer
(or alternatively select only the necessary pixels). Note, don't use more
than 1 core in the `predict()` function, because the `ranger` model uses
all threads by default.

```{r results="hide"}
landsat_pr = crop(landsat, ext(submission))
landsat_pr = predict(landsat_pr, mdl, index = 1, na.rm = TRUE, cores = 1)
```

Using the `levels()` function, we can assign categories to IDs.

```{r}
levels(landsat_pr) = levels(training)
plot(landsat_pr, main = "Prediction", col = colors)
```

The final step is to extract the land cover categories for the given coordinates
and save the result to a `.csv` file.

```{r}
pts_pr = extract(landsat_pr, submission, ID = FALSE)
write.csv(pts_pr, "test_submission.csv", row.names = FALSE)
```
