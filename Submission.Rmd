---
title: "Automatic land cover mapping"
author: "Krzysztof Dyba"
date: "28 August 2023"
output:
  html_document:
    df_print: paged
---

## Data loading

```{r message=FALSE}
library("terra")
library("ranger") # Random Forest model
library("yardstick") # validation metrics
set.seed(1) # define seed of randomness
```

Let's start by listing the rasters to be loaded from the `Landast_crop` folder.
We also need to define the file extension `pattern = "\\.TIF$"` and full paths
`full.names = TRUE` in `list.files()` function. Then we load these files using
the `rast()` function.

```{r}
landsat_f = list.files("Landast_crop/", pattern = "\\.TIF$", full.names = TRUE)
landsat_f = landsat_f[-1] # remove panchromatic band (15 m)
landsat = rast(landsat_f)
```

For ease of use, we can abbreviate the band names.

```{r}
names(landsat) = paste0("B", 1:7) # change band names
landsat
```

Next, we need to scale the values to the spectral reflectance. Basically,
we change the datatype from integer to float. Note that there are outliers
(below 0 and above 1) in this dataset.

```{r warning=FALSE}
# scale data to reflectance
landsat = landsat * 2.75e-05 - 0.2
summary(landsat) # print statistics
```

We can also display RGB composition using the `plotRGB()` function. We need to
properly define the spectral bands and it is worth to improve the contrast
using the `stretch` argument.

```{r}
plotRGB(landsat, r = 4, g = 3, b = 2, stretch = "lin")
```

Now let's load our reference data with land cover categories. We have two
datasets -- a training (to train the classification model) and a validation
(to independently evaluate its performance).

```{r}
training_f = "training_set.tif"
training = rast(training_f)
training
```

Using the `levels()` function we can see what categories occur in the area.

```{r}
levels(training)[[1]]
```

We can also display a map.

```{r}
colors = c("#1445f9", "#d20000", "#29a329", "#fdd327", "#d9d9d9")
plot(training, main = "Training dataset", col = colors)
```

We can load the validation data in exactly the same way.

```{r}
validation_f = "validation_set.tif"
validation = rast(validation_f)
validation
```

```{r}
plot(validation, main = "Validation dataset", col = colors)
```

Note two things:

1. The satellite data (EPSG:32634) and land cover data (EPSG:2180) have
different coordinate reference systems (CRS).
2. The rasters are not aligned (the pixels are slightly shifted).

In the last step, we load the csv file for which we have to make a prediction
and finally submit the results. For this purpose, we can use the `read.csv()`
function.

```{r}
submission = read.csv("submission.csv")
head(submission)
```

The first two columns contain the geographical coordinates of points in the
EPSG:2180 (longitude and latitude). The third column `category` is empty and
we have to indicate what land cover category is there.

Then let's convert the data frame to a vector (SpatVector) and reproject
the coordinate reference systems to EPSG:32634.

```{r}
submission = vect(submission, geom = c("X", "Y"), crs = "EPSG:2180")
submission = project(submission, crs(landsat))
```

## Model training

To train the model, we do not have to use all available data (pixels), we can
use a random sample, which will significantly speed up training.
The `spatSample()` function is used to draw the sample and can be done as follows.

```{r}
train_smp = spatSample(training, size = 20000, method = "random",
                       as.points = TRUE, values = FALSE, na.rm = TRUE)
```

We have already sample the points (coordinates), now we need to obtain the
values of spectral bands (`landsat`) and land cover categories (`training`
and `validation`). The `extract()` function is used to extract pixel values.
Remember that the objects must have identical CRS.

```{r}
x = extract(training, train_smp, ID = FALSE)
y = extract(landsat, project(train_smp, crs(landsat)), ID = FALSE)
train_smp = cbind(x, y) # combine columns
```

Let's check what the data distribution looks like using the `table()` and 
`prop.table()` functions.

```{r}
prop.table(table(train_smp$category)) * 100
```

As we can see, our dataset is strongly unbalanced, i.e. the most pixels
represent `cropland` and `vegetation`, and the other categories appear very
rarely.

We can use the same functions to create a validation dataset.

```{r}
validation_smp = spatSample(validation, size = 20000, method = "random",
                            as.points = TRUE, values = FALSE, na.rm = TRUE)
x = extract(validation, validation_smp, ID = FALSE)
y = extract(landsat, project(validation_smp, crs(landsat)), ID = FALSE)
validation_smp = cbind(x, y)
rm(x, y) # remove unnecessary variables
```

XXXXXXXXXXXXXXXXXXXXX

```{r}
mdl = ranger(category ~ ., data = train_smp, importance = "impurity")
```

```{r}
barplot(sort(importance(mdl)), xlab = "Spectral band",
        main = "Variable importance")
```

## Model evaluation

```{r collapse=TRUE}
validation_pr = predict(mdl, validation_smp[, -1])$predictions

# accuracy is over optimistic (don't use this)
accuracy_vec(validation_smp$category, validation_pr)

# balanced accuracy
bal_accuracy_vec(validation_smp$category, validation_pr)

# Cohen's kappa
kap_vec(validation_smp$category, validation_pr)
```

```{r}
# confusion matrix
table(prediction = validation_pr, true = validation_smp$category)
```

## Predict

```{r results="hide"}
landsat_pr = crop(landsat, ext(submission))
landsat_pr = predict(landsat_pr, mdl, index = 1, na.rm = TRUE, cores = 1)
```

```{r}
levels(landsat_pr) = levels(training)
plot(landsat_pr, main = "Prediction", col = colors)
```

```{r}
pts_pr = extract(landsat_pr, submission, ID = FALSE)
write.csv(pts_pr, "test_submission.csv", row.names = FALSE)
```
